{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.14.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\r\n",
      "\u001b[K     |████████████████████████████████| 377.0MB 31kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.12.0)\r\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (0.8.0)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (0.8.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.11.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.16.4)\r\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (3.10.0)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.24.3)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.0.8)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (0.33.6)\r\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\r\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 27.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (0.1.7)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\r\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==1.14.0) (0.2.2)\r\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\r\n",
      "\u001b[K     |████████████████████████████████| 491kB 41.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (41.4.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.9.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (0.16.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.1.1)\r\n",
      "\u001b[31mERROR: tensorflow 2.0.0 has requirement tensorboard<2.1.0,>=2.0.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: tensorflow 2.0.0 has requirement tensorflow-estimator<2.1.0,>=2.0.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\r\n",
      "  Found existing installation: tensorboard 2.0.0\r\n",
      "    Uninstalling tensorboard-2.0.0:\r\n",
      "      Successfully uninstalled tensorboard-2.0.0\r\n",
      "  Found existing installation: tensorflow-estimator 2.0.1\r\n",
      "    Uninstalling tensorflow-estimator-2.0.1:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.0.1\r\n",
      "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\r\n",
      "Collecting keras==2.2.4\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\r\n",
      "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (1.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (1.16.4)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (1.12.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (1.1.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (5.1.2)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras==2.2.4) (2.9.0)\r\n",
      "Installing collected packages: keras\r\n",
      "  Found existing installation: Keras 2.3.1\r\n",
      "    Uninstalling Keras-2.3.1:\r\n",
      "      Successfully uninstalled Keras-2.3.1\r\n",
      "Successfully installed keras-2.2.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.14.0\n",
    "!pip install keras==2.2.4\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n",
      "/kaggle/input/applications-of-deep-learningwustl-fall-2019/train.csv\n",
      "/kaggle/input/applications-of-deep-learningwustl-fall-2019/sample.csv\n",
      "/kaggle/input/applications-of-deep-learningwustl-fall-2019/test.csv\n",
      "/kaggle/input/quora-question-pairs/train.csv\n",
      "/kaggle/input/quora-question-pairs/sample_submission.csv\n",
      "/kaggle/input/quora-question-pairs/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import LSTM, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    # text = \"\".join([c for c in text if c not in punctuation])\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    #'../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n",
    "    '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "]\n",
    "NUM_MODELS = 2\n",
    "MAX_FEATURES = 1000000\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    words1 = Input(shape=(MAX_LEN,))\n",
    "    words2 = Input(shape=(MAX_LEN,))\n",
    "    magic_input = Input(shape=(train_leaks.shape[1],))\n",
    "    magic_dense = Dense(64, activation='relu')(magic_input)\n",
    "    x1 = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words1)\n",
    "    x2 = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words2)\n",
    "    x1 = SpatialDropout1D(0.25)(x1)\n",
    "    x2 = SpatialDropout1D(0.25)(x2)\n",
    "    x1 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x1)\n",
    "    x2 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x2)\n",
    "    x1 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=False))(x1)\n",
    "    x2 = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=False))(x2)\n",
    "    #x1 = CuDNNLSTM(LSTM_UNITS)(x1)\n",
    "    #x2 = CuDNNLSTM(LSTM_UNITS)(x2)\n",
    "    \n",
    "    hidden = concatenate([x1, x2, magic_dense])\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Dropout(0.25)(hidden)\n",
    "    hidden = Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Dropout(0.2)(hidden)\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=[words1, words2, magic_input], outputs=result)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(clipnorm=0.1),\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/applications-of-deep-learningwustl-fall-2019/train.csv')\n",
    "test = pd.read_csv('../input/applications-of-deep-learningwustl-fall-2019/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ex = pd.read_csv(\"../input/quora-question-pairs/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ex = train_ex[['id', 'question1', 'question2', 'is_duplicate']]\n",
    "train_ex = train_ex.rename(columns={\"question1\": \"sent1\", \"question2\": \"sent2\", \"is_duplicate\": \"same_source\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.append(train_ex, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_train = train['sent1'].fillna('').values\n",
    "s2_train = train['sent2'].fillna('').values\n",
    "y_train = train['same_source']\n",
    "s1_test = test['sent1'].fillna('').values\n",
    "s2_test = test['sent1'].fillna('').values\n",
    "\n",
    "x1_train, x2_train = [], []\n",
    "for line in s1_train:\n",
    "    x1_train.append(text_to_wordlist(line, remove_stopwords=False, stem_words=False))\n",
    "for line in s2_train:\n",
    "    x2_train.append(text_to_wordlist(line, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "x1_test, x2_test = [], []\n",
    "for line in s1_test:\n",
    "    x1_test.append(text_to_wordlist(line, remove_stopwords=False, stem_words=False))\n",
    "for line in s2_test:\n",
    "    x2_test.append(text_to_wordlist(line, remove_stopwords=False, stem_words=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(x1_train + x1_test + x2_train + x2_test)\n",
    "\n",
    "x1_train = tokenizer.texts_to_sequences(x1_train)\n",
    "x1_test = tokenizer.texts_to_sequences(x1_test)\n",
    "x2_train = tokenizer.texts_to_sequences(x2_train)\n",
    "x2_test = tokenizer.texts_to_sequences(x2_test)\n",
    "\n",
    "x1_train = sequence.pad_sequences(x1_train, maxlen=MAX_LEN)\n",
    "x1_test = sequence.pad_sequences(x1_test, maxlen=MAX_LEN)\n",
    "x2_train = sequence.pad_sequences(x2_train, maxlen=MAX_LEN)\n",
    "x2_test = sequence.pad_sequences(x2_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.concat([train[['sent1', 'sent2']], \\\n",
    "        test[['sent1', 'sent2']]], axis=0).reset_index(drop='index')\n",
    "s_dict = collections.defaultdict(set)\n",
    "for i in range(sentences.shape[0]):\n",
    "        s_dict[sentences.sent1[i]].add(sentences.sent2[i])\n",
    "        s_dict[sentences.sent2[i]].add(sentences.sent1[i])\n",
    "\n",
    "def s1_freq(row):\n",
    "    return(len(s_dict[row['sent1']]))\n",
    "    \n",
    "def s2_freq(row):\n",
    "    return(len(s_dict[row['sent2']]))\n",
    "    \n",
    "def s1_s2_intersect(row):\n",
    "    return(len(set(s_dict[row['sent1']]).intersection(set(s_dict[row['sent2']]))))\n",
    "\n",
    "train['s1_s2_intersect'] = train.apply(s1_s2_intersect, axis=1, raw=True)\n",
    "train['s1_freq'] = train.apply(s1_freq, axis=1, raw=True)\n",
    "train['s2_freq'] = train.apply(s2_freq, axis=1, raw=True)\n",
    "\n",
    "test['s1_s2_intersect'] = test.apply(s1_s2_intersect, axis=1, raw=True)\n",
    "test['s1_freq'] = test.apply(s1_freq, axis=1, raw=True)\n",
    "test['s2_freq'] = test.apply(s2_freq, axis=1, raw=True)\n",
    "\n",
    "train_leaks = train[['s1_s2_intersect', 's1_freq', 's2_freq']]\n",
    "test_leaks = test[['s1_s2_intersect', 's1_freq', 's2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((train_leaks, test_leaks)))\n",
    "train_leaks = ss.transform(train_leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 453429 samples, validate on 80017 samples\n",
      "Epoch 1/50\n",
      "453429/453429 [==============================] - 106s 233us/step - loss: 0.3332 - acc: 0.8600 - val_loss: 0.3544 - val_acc: 0.8476\n",
      "Epoch 2/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2869 - acc: 0.8785 - val_loss: 0.3342 - val_acc: 0.8560\n",
      "Epoch 3/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2697 - acc: 0.8840 - val_loss: 0.3119 - val_acc: 0.8600\n",
      "Epoch 4/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2565 - acc: 0.8893 - val_loss: 0.3059 - val_acc: 0.8631\n",
      "Epoch 5/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2465 - acc: 0.8930 - val_loss: 0.3048 - val_acc: 0.8635\n",
      "Epoch 6/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2376 - acc: 0.8967 - val_loss: 0.2957 - val_acc: 0.8693\n",
      "Epoch 7/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2284 - acc: 0.9005 - val_loss: 0.2925 - val_acc: 0.8690\n",
      "Epoch 8/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2201 - acc: 0.9037 - val_loss: 0.3020 - val_acc: 0.8682\n",
      "Epoch 9/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.2118 - acc: 0.9073 - val_loss: 0.3009 - val_acc: 0.8631\n",
      "Epoch 10/50\n",
      "453429/453429 [==============================] - 99s 217us/step - loss: 0.2033 - acc: 0.9115 - val_loss: 0.3065 - val_acc: 0.8597\n",
      "Epoch 11/50\n",
      "453429/453429 [==============================] - 99s 219us/step - loss: 0.1948 - acc: 0.9153 - val_loss: 0.3021 - val_acc: 0.8686\n",
      "Epoch 12/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.1873 - acc: 0.9184 - val_loss: 0.3127 - val_acc: 0.8701\n",
      "Epoch 13/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.1799 - acc: 0.9221 - val_loss: 0.3065 - val_acc: 0.8620\n",
      "Epoch 14/50\n",
      "453429/453429 [==============================] - 99s 218us/step - loss: 0.1747 - acc: 0.9242 - val_loss: 0.3231 - val_acc: 0.8574\n",
      "Epoch 15/50\n",
      "185856/453429 [===========>..................] - ETA: 54s - loss: 0.1632 - acc: 0.9293"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "#for model_idx in range(NUM_MODELS):\n",
    "model = build_model(embedding_matrix)\n",
    "    #for global_epoch in range(EPOCHS):\n",
    "model.fit(\n",
    "    [x1_train, x2_train, train_leaks],\n",
    "    y_train,\n",
    "    validation_split = 0.15,\n",
    "    shuffle = True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10)\n",
    "        #LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "    ]\n",
    ")\n",
    "        #checkpoint_predictions.append(model.predict([x1_test, x2_test], batch_size=2048)[0].flatten())\n",
    "        #weights.append(2 ** global_epoch)\n",
    "\n",
    "#predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "predictions = model.predict([x1_test, x2_test, test_leaks], batch_size=2048) + model.predict([x2_test, x1_test, test_leaks], batch_size=2048)\n",
    "predictions /= 2\n",
    "predictions = predictions.ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'same_source': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14350.000000\n",
       "mean         0.510810\n",
       "std          0.480405\n",
       "min          0.000000\n",
       "25%          0.000016\n",
       "50%          0.641065\n",
       "75%          0.998141\n",
       "max          1.000000\n",
       "Name: same_source, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.same_source.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
